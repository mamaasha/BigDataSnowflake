{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76f85fb4-f586-485d-ad78-c7eb09bbdc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úîÔ∏è SparkSession –∑–∞–ø—É—â–µ–Ω\n"
     ]
    }
   ],
   "source": [
    "# –Ø—á–µ–π–∫–∞ 1: –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Spark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, year, quarter, month, dayofmonth, date_format\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark ETL Star Schema\") \\\n",
    "    .config(\"spark.jars\", \"postgresql-42.6.0.jar\") \\\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "jdbc_url = \"jdbc:postgresql://postgres:5432/spark_db\"\n",
    "pg_props = {\n",
    "    \"user\": \"spark_user\",\n",
    "    \"password\": \"spark_password\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "print(\"‚úîÔ∏è SparkSession –∑–∞–ø—É—â–µ–Ω\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f5f22e8-230e-406e-a02b-a9c69f4b4ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• –ò–∑ mock_data –ø—Ä–æ—á–∏—Ç–∞–Ω–æ 10000 —Å—Ç—Ä–æ–∫\n"
     ]
    }
   ],
   "source": [
    "# –Ø—á–µ–π–∫–∞ 2: —á–∏—Ç–∞–µ–º —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ —Å—á–∏—Ç–∞–µ–º —Å—Ç—Ä–æ–∫–∏\n",
    "df_raw = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"dbtable\", \"mock_data\") \\\n",
    "    .options(**pg_props) \\\n",
    "    .load()\n",
    "\n",
    "total = df_raw.count()\n",
    "print(f\"üì• –ò–∑ mock_data –ø—Ä–æ—á–∏—Ç–∞–Ω–æ {total} —Å—Ç—Ä–æ–∫\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9019d0dd-f1da-4067-b300-b120f6341158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ dim_countries...\n",
      "   üîç –ù–∞–π–¥–µ–Ω–æ 230 —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Ç—Ä–∞–Ω –≤ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
      "   ‚ûï –ë—É–¥—É—Ç –≤—Å—Ç–∞–≤–ª–µ–Ω—ã 0 –Ω–æ–≤—ã—Ö —Å—Ç—Ä–æ–∫ –≤ dim_countries\n",
      "   ‚ö†Ô∏è –ù–µ—á–µ–≥–æ –≤—Å—Ç–∞–≤–ª—è—Ç—å –≤ dim_countries\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# dim_countries\n",
    "# -------------------------------\n",
    "print(\"üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ dim_countries...\")\n",
    "\n",
    "# 1) –°–æ–±–∏—Ä–∞–µ–º –Ω–æ–≤—ã–µ —Å—Ç—Ä–∞–Ω—ã –∏–∑ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "df_cust   = df_raw.select(col(\"customer_country\").alias(\"country_name\"))\n",
    "df_sell   = df_raw.select(col(\"seller_country\").alias(\"country_name\"))\n",
    "df_store  = df_raw.select(col(\"store_country\").alias(\"country_name\"))\n",
    "df_supp   = df_raw.select(col(\"supplier_country\").alias(\"country_name\"))\n",
    "\n",
    "df_countries_new = (\n",
    "    df_cust.union(df_sell)\n",
    "           .union(df_store)\n",
    "           .union(df_supp)\n",
    "           .where(col(\"country_name\").isNotNull())\n",
    "           .distinct()\n",
    ")\n",
    "new_cnt = df_countries_new.count()\n",
    "print(f\"   üîç –ù–∞–π–¥–µ–Ω–æ {new_cnt} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Ç—Ä–∞–Ω –≤ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö\")\n",
    "\n",
    "# 2) –ß–∏—Ç–∞–µ–º —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Å—Ç—Ä–∞–Ω—ã –∏–∑ Postgres (–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Å–∏–Ω—Ç–∞–∫—Å–∏—Å)\n",
    "df_exist = spark.read.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"dim_countries\",\n",
    "    properties=pg_props\n",
    ")\n",
    "\n",
    "# 3) ¬´–ê–Ω—Ç–∏-–¥–∂–æ–∏–Ω¬ª –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –Ω–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π\n",
    "to_insert = df_countries_new.join(df_exist, [\"country_name\"], \"left_anti\")\n",
    "ins_cnt = to_insert.count()\n",
    "print(f\"   ‚ûï –ë—É–¥—É—Ç –≤—Å—Ç–∞–≤–ª–µ–Ω—ã {ins_cnt} –Ω–æ–≤—ã—Ö —Å—Ç—Ä–æ–∫ –≤ dim_countries\")\n",
    "\n",
    "# 4) –ó–∞–ø–∏—Å—å –Ω–æ–≤—ã—Ö —Å—Ç—Ä–æ–∫ (–µ—Å–ª–∏ –µ—Å—Ç—å)\n",
    "if ins_cnt > 0:\n",
    "    to_insert.write.jdbc(\n",
    "        url=jdbc_url,\n",
    "        table=\"dim_countries\",\n",
    "        mode=\"append\",\n",
    "        properties=pg_props\n",
    "    )\n",
    "    print(\"   ‚úÖ –í—Å—Ç–∞–≤–∫–∞ –≤ dim_countries –∑–∞–≤–µ—Ä—à–µ–Ω–∞\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è –ù–µ—á–µ–≥–æ –≤—Å—Ç–∞–≤–ª—è—Ç—å –≤ dim_countries\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd7748b-a4e5-4876-9623-5adb8bb61e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# dim_cities\n",
    "# -------------------------------\n",
    "print(\"üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ dim_cities...\")\n",
    "df_cities_new = (\n",
    "    df_raw.select(col(\"store_city\").alias(\"city_name\"))\n",
    "          .union(df_raw.select(col(\"supplier_city\").alias(\"city_name\")))\n",
    "          .where(col(\"city_name\").isNotNull())\n",
    "          .distinct()\n",
    ")\n",
    "print(f\"   üîç –ù–∞–π–¥–µ–Ω–æ {df_cities_new.count()} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –≥–æ—Ä–æ–¥–æ–≤\")\n",
    "\n",
    "df_exist = spark.read.jdbc(url=jdbc_url, table=\"dim_cities\", properties=pg_props)\n",
    "to_insert = df_cities_new.join(df_exist, [\"city_name\"], \"left_anti\")\n",
    "print(f\"   ‚ûï –ë—É–¥—É—Ç –≤—Å—Ç–∞–≤–ª–µ–Ω—ã {to_insert.count()} –Ω–æ–≤—ã—Ö —Å—Ç—Ä–æ–∫ –≤ dim_cities\")\n",
    "if to_insert.count() > 0:\n",
    "    to_insert.write.jdbc(url=jdbc_url, table=\"dim_cities\", mode=\"append\", properties=pg_props)\n",
    "    print(\"   ‚úÖ –í—Å—Ç–∞–≤–∫–∞ –≤ dim_cities –∑–∞–≤–µ—Ä—à–µ–Ω–∞\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è –ù–µ—á–µ–≥–æ –≤—Å—Ç–∞–≤–ª—è—Ç—å –≤ dim_cities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d4561c-f297-4ce0-b8b0-226735ea8a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# dim_dates\n",
    "# -------------------------------\n",
    "print(\"üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ dim_dates...\")\n",
    "df_dates = (\n",
    "    df_raw.select(to_date(\"sale_date\",            \"MM/dd/yyyy\").alias(\"full_date\"))\n",
    "          .union(df_raw.select(to_date(\"product_release_date\", \"MM/dd/yyyy\")))\n",
    "          .union(df_raw.select(to_date(\"product_expiry_date\",  \"MM/dd/yyyy\")))\n",
    "          .where(col(\"full_date\").isNotNull())\n",
    "          .distinct()\n",
    "          .withColumn(\"year\",    year(\"full_date\"))\n",
    "          .withColumn(\"quarter\", quarter(\"full_date\"))\n",
    "          .withColumn(\"month\",   month(\"full_date\"))\n",
    "          .withColumn(\"day\",     dayofmonth(\"full_date\"))\n",
    "          .withColumn(\"weekday\", date_format(\"full_date\", \"EEEE\"))\n",
    ")\n",
    "print(f\"   üîç –ù–∞–π–¥–µ–Ω–æ {df_dates.count()} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –¥–∞—Ç\")\n",
    "\n",
    "df_exist = spark.read.jdbc(url=jdbc_url, table=\"dim_dates\", properties=pg_props)\n",
    "to_insert = df_dates.join(df_exist, [\"full_date\"], \"left_anti\")\n",
    "print(f\"   ‚ûï –ë—É–¥—É—Ç –≤—Å—Ç–∞–≤–ª–µ–Ω—ã {to_insert.count()} –Ω–æ–≤—ã—Ö —Å—Ç—Ä–æ–∫ –≤ dim_dates\")\n",
    "if to_insert.count() > 0:\n",
    "    to_insert.write.jdbc(url=jdbc_url, table=\"dim_dates\", mode=\"append\", properties=pg_props)\n",
    "    print(\"   ‚úÖ –í—Å—Ç–∞–≤–∫–∞ –≤ dim_dates –∑–∞–≤–µ—Ä—à–µ–Ω–∞\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è –ù–µ—á–µ–≥–æ –≤—Å—Ç–∞–≤–ª—è—Ç—å –≤ dim_dates\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e4d90e-1e1b-4772-b5a5-1c0417584343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# –ü—Ä–æ—Å—Ç—ã–µ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∏: pet types, breeds, categories\n",
    "# -------------------------------\n",
    "dims = [\n",
    "    (\"customer_pet_type\",  \"dim_pet_types\",       \"pet_type_name\"),\n",
    "    (\"customer_pet_breed\", \"dim_pet_breeds\",      \"pet_breed_name\"),\n",
    "    (\"pet_category\",       \"dim_pet_categories\",  \"pet_category_name\"),\n",
    "]\n",
    "\n",
    "for col_src, table, name_col in dims:\n",
    "    print(f\"üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ {table}...\")\n",
    "    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏ –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º –∫–æ–ª–æ–Ω–∫—É —Å—Ä–∞–∑—É –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –∏–º—è\n",
    "    df_new = (\n",
    "        df_raw\n",
    "        .select(col(col_src).alias(name_col))\n",
    "        .where(col(name_col).isNotNull())\n",
    "        .distinct()\n",
    "    )\n",
    "    new_cnt = df_new.count()\n",
    "    print(f\"   üîç –ù–∞–π–¥–µ–Ω–æ {new_cnt} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –≤ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö\")\n",
    "\n",
    "    df_exist = spark.read.jdbc(url=jdbc_url, table=table, properties=pg_props)\n",
    "    to_insert = df_new.join(df_exist, [name_col], \"left_anti\")\n",
    "    ins_cnt = to_insert.count()\n",
    "    print(f\"   ‚ûï –ë—É–¥—É—Ç –≤—Å—Ç–∞–≤–ª–µ–Ω—ã {ins_cnt} –Ω–æ–≤—ã—Ö —Å—Ç—Ä–æ–∫ –≤ {table}\")\n",
    "\n",
    "    if ins_cnt > 0:\n",
    "        to_insert.write.jdbc(\n",
    "            url=jdbc_url,\n",
    "            table=table,\n",
    "            mode=\"append\",\n",
    "            properties=pg_props\n",
    "        )\n",
    "        print(f\"   ‚úÖ –í—Å—Ç–∞–≤–∫–∞ –≤ {table} –∑–∞–≤–µ—Ä—à–µ–Ω–∞\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è –ù–µ—á–µ–≥–æ –≤—Å—Ç–∞–≤–ª—è—Ç—å –≤ {table}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "19ed28e1-2e83-47d2-a8fd-a4d8e1e5eeaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ dim_pets...\n",
      "   üîç –í —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–∞–π–¥–µ–Ω–æ 9850 —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–∏—Ç–æ–º—Ü–µ–≤ (–∏–º—è+—Ç–∏–ø)\n",
      "   üîç –ü–æ—Å–ª–µ –¥–∂–æ–π–Ω–∞ —Å–æ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∞–º–∏ –∏ dedupe –ø–æ–ª—É—á–µ–Ω–æ 9850 –∑–∞–ø–∏—Å–µ–π\n",
      "   ‚ûï –ë—É–¥—É—Ç –≤—Å—Ç–∞–≤–ª–µ–Ω—ã 0 –Ω–æ–≤—ã—Ö —Å—Ç—Ä–æ–∫ –≤ dim_pets\n",
      "   ‚ö†Ô∏è –ù–µ—á–µ–≥–æ –≤—Å—Ç–∞–≤–ª—è—Ç—å –≤ dim_pets\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# dim_pets (—Å –≤—ã–≤–æ–¥–æ–º –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π)\n",
    "# -------------------------------\n",
    "print(\"üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ dim_pets...\")\n",
    "\n",
    "# 1) –°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∫–ª—é—á–∏\n",
    "df_exist = spark.read.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"dim_pets\",\n",
    "    properties=pg_props\n",
    ").select(\"pet_name\", \"pet_type_id\")\n",
    "\n",
    "# 2) –§–æ—Ä–º–∏—Ä—É–µ–º –Ω–æ–≤—ã–µ –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ —Å—Ä–∞–∑—É distinct\n",
    "df_pets_new = (\n",
    "    df_raw\n",
    "      .select(\n",
    "         col(\"customer_pet_name\").alias(\"pet_name\"),\n",
    "         col(\"customer_pet_type\").alias(\"pet_type_name\"),\n",
    "         col(\"customer_pet_breed\").alias(\"pet_breed_name\"),\n",
    "         col(\"pet_category\").alias(\"pet_category_name\")\n",
    "      )\n",
    "      .where(col(\"pet_name\").isNotNull())\n",
    "      .distinct()\n",
    ")\n",
    "\n",
    "new_raw_cnt = df_pets_new.count()\n",
    "print(f\"   üîç –í —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–∞–π–¥–µ–Ω–æ {new_raw_cnt} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–∏—Ç–æ–º—Ü–µ–≤ (–∏–º—è+—Ç–∏–ø)\")\n",
    "\n",
    "# 3) –ü–æ–¥—Ç—è–≥–∏–≤–∞–µ–º —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∏\n",
    "dim_pet_types  = spark.read.jdbc(url=jdbc_url, table=\"dim_pet_types\",  properties=pg_props)\n",
    "dim_pet_breeds = spark.read.jdbc(url=jdbc_url, table=\"dim_pet_breeds\", properties=pg_props)\n",
    "dim_pet_cats   = spark.read.jdbc(url=jdbc_url, table=\"dim_pet_categories\", properties=pg_props)\n",
    "\n",
    "df_pets = (\n",
    "    df_pets_new\n",
    "      .join(dim_pet_types,  [\"pet_type_name\"],    \"left\")\n",
    "      .join(dim_pet_breeds, [\"pet_breed_name\"],   \"left\")\n",
    "      .join(dim_pet_cats,   [\"pet_category_name\"],\"left\")\n",
    "      .select(\"pet_name\", \"pet_type_id\", \"pet_breed_id\", \"pet_category_id\")\n",
    "      .distinct()\n",
    ")\n",
    "\n",
    "new_cnt = df_pets.count()\n",
    "print(f\"   üîç –ü–æ—Å–ª–µ –¥–∂–æ–π–Ω–∞ —Å–æ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∞–º–∏ –∏ dedupe –ø–æ–ª—É—á–µ–Ω–æ {new_cnt} –∑–∞–ø–∏—Å–µ–π\")\n",
    "\n",
    "# 4) –ê–Ω—Ç–∏-–¥–∂–æ–∏–Ω –ø—Ä–æ—Ç–∏–≤ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö\n",
    "to_insert = df_pets.join(df_exist, [\"pet_name\", \"pet_type_id\"], \"left_anti\")\n",
    "ins_cnt   = to_insert.count()\n",
    "print(f\"   ‚ûï –ë—É–¥—É—Ç –≤—Å—Ç–∞–≤–ª–µ–Ω—ã {ins_cnt} –Ω–æ–≤—ã—Ö —Å—Ç—Ä–æ–∫ –≤ dim_pets\")\n",
    "\n",
    "# 5) –ó–∞–ø–∏—Å—å (–µ—Å–ª–∏ –µ—Å—Ç—å)\n",
    "if ins_cnt > 0:\n",
    "    to_insert.write.jdbc(\n",
    "        url=jdbc_url,\n",
    "        table=\"dim_pets\",\n",
    "        mode=\"append\",\n",
    "        properties=pg_props\n",
    "    )\n",
    "    print(\"   ‚úÖ –í—Å—Ç–∞–≤–∫–∞ –≤ dim_pets –∑–∞–≤–µ—Ä—à–µ–Ω–∞\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è –ù–µ—á–µ–≥–æ –≤—Å—Ç–∞–≤–ª—è—Ç—å –≤ dim_pets\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "94bbf5a0-ff8f-4f95-aeba-4a59ca25edb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ dim_suppliers...\n",
      "   üîç –í —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–∞–π–¥–µ–Ω–æ 10000 —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–æ—Å—Ç–∞–≤—â–∏–∫–æ–≤ –ø–æ email\n",
      "   üîç –ü–æ—Å–ª–µ –¥–∂–æ–π–Ω–∞ —Å–æ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∞–º–∏ –ø–æ–ª—É—á–µ–Ω–æ 10000 –∑–∞–ø–∏—Å–µ–π –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏\n",
      "   ‚ûï –ë—É–¥—É—Ç –≤—Å—Ç–∞–≤–ª–µ–Ω—ã 10000 –Ω–æ–≤—ã—Ö —Å—Ç—Ä–æ–∫ –≤ dim_suppliers\n",
      "   ‚úÖ –í—Å—Ç–∞–≤–∫–∞ –≤ dim_suppliers –∑–∞–≤–µ—Ä—à–µ–Ω–∞\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# dim_suppliers (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –±–ª–æ–∫)\n",
    "# -------------------------------\n",
    "print(\"üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ dim_suppliers...\")\n",
    "\n",
    "# 1) –ß–∏—Ç–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∫–ª—é—á–∏ (–ø–æ —É–Ω–∏–∫–∞–ª—å–Ω–æ–º—É email)\n",
    "df_exist = spark.read.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"dim_suppliers\",\n",
    "    properties=pg_props\n",
    ").select(\"supplier_email\")\n",
    "\n",
    "# 2) –§–æ—Ä–º–∏—Ä—É–µ–º –Ω–æ–≤—ã–µ –∑–∞–ø–∏—Å–∏ –∏–∑ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "df_supp_new = (\n",
    "    df_raw\n",
    "      .select(\n",
    "         col(\"supplier_name\"),\n",
    "         col(\"supplier_contact\").alias(\"contact_person\"),  # –ø—Ä–∞–≤–∏–ª—å–Ω–æ –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–ª–∏\n",
    "         col(\"supplier_email\"),\n",
    "         col(\"supplier_phone\"),\n",
    "         col(\"supplier_address\"),\n",
    "         col(\"supplier_city\").alias(\"city_name\"),\n",
    "         col(\"supplier_country\").alias(\"country_name\")\n",
    "      )\n",
    "      .where(col(\"supplier_email\").isNotNull())\n",
    "      .distinct()\n",
    ")\n",
    "\n",
    "new_cnt = df_supp_new.count()\n",
    "print(f\"   üîç –í —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–∞–π–¥–µ–Ω–æ {new_cnt} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–æ—Å—Ç–∞–≤—â–∏–∫–æ–≤ –ø–æ email\")\n",
    "\n",
    "# 3) –ü–æ–¥—Ç—è–≥–∏–≤–∞–µ–º –≤–Ω–µ—à–Ω–∏–µ –∫–ª—é—á–∏: city_id –∏ country_id\n",
    "dim_cities    = spark.read.jdbc(url=jdbc_url, table=\"dim_cities\",    properties=pg_props)\n",
    "dim_countries = spark.read.jdbc(url=jdbc_url, table=\"dim_countries\", properties=pg_props)\n",
    "\n",
    "df_supp = (\n",
    "    df_supp_new\n",
    "      .join(dim_cities,    df_supp_new.city_name    == dim_cities.city_name,       \"left\")\n",
    "      .join(dim_countries, df_supp_new.country_name == dim_countries.country_name, \"left\")\n",
    "      .select(\n",
    "          \"supplier_name\",\n",
    "          \"contact_person\",\n",
    "          \"supplier_email\",\n",
    "          \"supplier_phone\",\n",
    "          \"supplier_address\",\n",
    "          col(\"city_id\"),\n",
    "          col(\"country_id\")\n",
    "      )\n",
    ")\n",
    "\n",
    "post_join_cnt = df_supp.count()\n",
    "print(f\"   üîç –ü–æ—Å–ª–µ –¥–∂–æ–π–Ω–∞ —Å–æ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∞–º–∏ –ø–æ–ª—É—á–µ–Ω–æ {post_join_cnt} –∑–∞–ø–∏—Å–µ–π –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏\")\n",
    "\n",
    "# 4) –ê–Ω—Ç–∏-–¥–∂–æ–∏–Ω –ø—Ä–æ—Ç–∏–≤ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ø–æ supplier_email\n",
    "to_insert = df_supp.join(df_exist, [\"supplier_email\"], \"left_anti\")\n",
    "ins_cnt   = to_insert.count()\n",
    "print(f\"   ‚ûï –ë—É–¥—É—Ç –≤—Å—Ç–∞–≤–ª–µ–Ω—ã {ins_cnt} –Ω–æ–≤—ã—Ö —Å—Ç—Ä–æ–∫ –≤ dim_suppliers\")\n",
    "\n",
    "# 5) –ó–∞–ø–∏—Å—å (–µ—Å–ª–∏ –µ—Å—Ç—å)\n",
    "if ins_cnt > 0:\n",
    "    to_insert.write.jdbc(\n",
    "        url=jdbc_url,\n",
    "        table=\"dim_suppliers\",\n",
    "        mode=\"append\",\n",
    "        properties=pg_props\n",
    "    )\n",
    "    print(\"   ‚úÖ –í—Å—Ç–∞–≤–∫–∞ –≤ dim_suppliers –∑–∞–≤–µ—Ä—à–µ–Ω–∞\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è –ù–µ—á–µ–≥–æ –≤—Å—Ç–∞–≤–ª—è—Ç—å –≤ dim_suppliers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2f478079-86f0-4723-b4fc-a43d80bc3db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ dim_product_categories...\n",
      "   üîç –ù–∞–π–¥–µ–Ω–æ 3 —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π\n",
      "   ‚ûï –ë—É–¥—É—Ç –≤—Å—Ç–∞–≤–ª–µ–Ω—ã 3 –Ω–æ–≤—ã—Ö —Å—Ç—Ä–æ–∫ –≤ dim_product_categories\n",
      "   ‚úÖ –í—Å—Ç–∞–≤–∫–∞ –≤ dim_product_categories –∑–∞–≤–µ—Ä—à–µ–Ω–∞\n",
      "üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ dim_product_colors...\n",
      "   üîç –ù–∞–π–¥–µ–Ω–æ 19 —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π\n",
      "   ‚ûï –ë—É–¥—É—Ç –≤—Å—Ç–∞–≤–ª–µ–Ω—ã 19 –Ω–æ–≤—ã—Ö —Å—Ç—Ä–æ–∫ –≤ dim_product_colors\n",
      "   ‚úÖ –í—Å—Ç–∞–≤–∫–∞ –≤ dim_product_colors –∑–∞–≤–µ—Ä—à–µ–Ω–∞\n",
      "üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ dim_product_sizes...\n",
      "   üîç –ù–∞–π–¥–µ–Ω–æ 3 —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π\n",
      "   ‚ûï –ë—É–¥—É—Ç –≤—Å—Ç–∞–≤–ª–µ–Ω—ã 3 –Ω–æ–≤—ã—Ö —Å—Ç—Ä–æ–∫ –≤ dim_product_sizes\n",
      "   ‚úÖ –í—Å—Ç–∞–≤–∫–∞ –≤ dim_product_sizes –∑–∞–≤–µ—Ä—à–µ–Ω–∞\n",
      "üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ dim_product_brands...\n",
      "   üîç –ù–∞–π–¥–µ–Ω–æ 383 —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π\n",
      "   ‚ûï –ë—É–¥—É—Ç –≤—Å—Ç–∞–≤–ª–µ–Ω—ã 383 –Ω–æ–≤—ã—Ö —Å—Ç—Ä–æ–∫ –≤ dim_product_brands\n",
      "   ‚úÖ –í—Å—Ç–∞–≤–∫–∞ –≤ dim_product_brands –∑–∞–≤–µ—Ä—à–µ–Ω–∞\n",
      "üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ dim_product_materials...\n",
      "   üîç –ù–∞–π–¥–µ–Ω–æ 11 —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π\n",
      "   ‚ûï –ë—É–¥—É—Ç –≤—Å—Ç–∞–≤–ª–µ–Ω—ã 11 –Ω–æ–≤—ã—Ö —Å—Ç—Ä–æ–∫ –≤ dim_product_materials\n",
      "   ‚úÖ –í—Å—Ç–∞–≤–∫–∞ –≤ dim_product_materials –∑–∞–≤–µ—Ä—à–µ–Ω–∞\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# –ü—Ä–æ–¥—É–∫—Ç–æ–≤—ã–µ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∏: category/color/size/brand/material\n",
    "# -------------------------------\n",
    "for src,col_name,table in [\n",
    "    (\"product_category\",\"category_name\",\"dim_product_categories\"),\n",
    "    (\"product_color\",   \"color_name\",   \"dim_product_colors\"),\n",
    "    (\"product_size\",    \"size_name\",    \"dim_product_sizes\"),\n",
    "    (\"product_brand\",   \"brand_name\",   \"dim_product_brands\"),\n",
    "    (\"product_material\",\"material_name\",\"dim_product_materials\"),\n",
    "]:\n",
    "    print(f\"üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ {table}...\")\n",
    "    df_new = (\n",
    "        df_raw.select(col(src).alias(col_name))\n",
    "              .where(col(col_name).isNotNull())\n",
    "              .distinct()\n",
    "    )\n",
    "    print(f\"   üîç –ù–∞–π–¥–µ–Ω–æ {df_new.count()} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π\")\n",
    "\n",
    "    df_exist = spark.read.jdbc(url=jdbc_url, table=table, properties=pg_props)\n",
    "    to_insert = df_new.join(df_exist, [col_name], \"left_anti\")\n",
    "    print(f\"   ‚ûï –ë—É–¥—É—Ç –≤—Å—Ç–∞–≤–ª–µ–Ω—ã {to_insert.count()} –Ω–æ–≤—ã—Ö —Å—Ç—Ä–æ–∫ –≤ {table}\")\n",
    "    if to_insert.count() > 0:\n",
    "        to_insert.write.jdbc(url=jdbc_url, table=table, mode=\"append\", properties=pg_props)\n",
    "        print(f\"   ‚úÖ –í—Å—Ç–∞–≤–∫–∞ –≤ {table} –∑–∞–≤–µ—Ä—à–µ–Ω–∞\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è –ù–µ—á–µ–≥–æ –≤—Å—Ç–∞–≤–ª—è—Ç—å –≤ {table}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5dd01bfe-755c-4c58-b8e9-4ef3c5184313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ dim_products...\n",
      "   ‚ûï –ë—É–¥—É—Ç –≤—Å—Ç–∞–≤–ª–µ–Ω—ã 10000 –Ω–æ–≤—ã—Ö —Å—Ç—Ä–æ–∫ –≤ dim_products\n",
      "   ‚úÖ –í—Å—Ç–∞–≤–∫–∞ –≤ dim_products –∑–∞–≤–µ—Ä—à–µ–Ω–∞\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# dim_products (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –±–ª–æ–∫)\n",
    "# -------------------------------\n",
    "print(\"üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ dim_products...\")\n",
    "\n",
    "# 1) –ß–∏—Ç–∞–µ–º —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∏\n",
    "dim_prod_cat = spark.read.jdbc(url=jdbc_url, table=\"dim_product_categories\", properties=pg_props)\n",
    "dim_colors   = spark.read.jdbc(url=jdbc_url, table=\"dim_product_colors\",     properties=pg_props)\n",
    "dim_sizes    = spark.read.jdbc(url=jdbc_url, table=\"dim_product_sizes\",      properties=pg_props)\n",
    "dim_brands   = spark.read.jdbc(url=jdbc_url, table=\"dim_product_brands\",     properties=pg_props)\n",
    "dim_mats     = spark.read.jdbc(url=jdbc_url, table=\"dim_product_materials\",  properties=pg_props)\n",
    "dim_suppliers= spark.read.jdbc(url=jdbc_url, table=\"dim_suppliers\",          properties=pg_props)\n",
    "\n",
    "# –ß–∏—Ç–∞–µ–º dim_dates –∏ –¥–∞—ë–º –¥–≤–∞ —Ä–∞–∑–Ω—ã—Ö –∞–ª–∏–∞—Å–∞\n",
    "dim_dates_full = spark.read.jdbc(url=jdbc_url, table=\"dim_dates\", properties=pg_props)\n",
    "dim_dates_rel  = dim_dates_full.select(\n",
    "    col(\"date_id\").alias(\"release_date_id\"),\n",
    "    col(\"full_date\").alias(\"release_full_date\")\n",
    ")\n",
    "dim_dates_exp  = dim_dates_full.select(\n",
    "    col(\"date_id\").alias(\"expiry_date_id\"),\n",
    "    col(\"full_date\").alias(\"expiry_full_date\")\n",
    ")\n",
    "\n",
    "# 2) –§–æ—Ä–º–∏—Ä—É–µ–º –Ω–æ–≤—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã\n",
    "df_products_raw = (\n",
    "    df_raw\n",
    "      .select(\n",
    "         col(\"product_name\"),\n",
    "         col(\"supplier_email\"),\n",
    "         col(\"product_category\").alias(\"category_name\"),\n",
    "         col(\"product_price\").alias(\"price\"),\n",
    "         col(\"product_weight\").alias(\"weight\"),\n",
    "         col(\"product_color\").alias(\"color_name\"),\n",
    "         col(\"product_size\").alias(\"size_name\"),\n",
    "         col(\"product_brand\").alias(\"brand_name\"),\n",
    "         col(\"product_material\").alias(\"material_name\"),\n",
    "         col(\"product_description\").alias(\"description\"),\n",
    "         col(\"product_rating\").alias(\"rating\"),\n",
    "         col(\"product_reviews\").alias(\"reviews\"),\n",
    "         to_date(\"product_release_date\", \"MM/dd/yyyy\").alias(\"release_full_date\"),\n",
    "         to_date(\"product_expiry_date\",  \"MM/dd/yyyy\").alias(\"expiry_full_date\")\n",
    "      )\n",
    "      .where(col(\"product_name\").isNotNull())\n",
    "      .distinct()\n",
    ")\n",
    "\n",
    "# 3) –î–∂–æ–∏–Ω–∏–º –≤—Å–µ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∏ –ø–æ –∏–º–µ–Ω–∞–º\n",
    "df_products = (\n",
    "    df_products_raw\n",
    "      .join(dim_prod_cat, df_products_raw.category_name == dim_prod_cat.category_name, \"left\")\n",
    "      .join(dim_colors,   df_products_raw.color_name    == dim_colors.color_name,   \"left\")\n",
    "      .join(dim_sizes,    df_products_raw.size_name     == dim_sizes.size_name,     \"left\")\n",
    "      .join(dim_brands,   df_products_raw.brand_name    == dim_brands.brand_name,   \"left\")\n",
    "      .join(dim_mats,     df_products_raw.material_name == dim_mats.material_name,  \"left\")\n",
    "      .join(dim_suppliers,df_products_raw.supplier_email == dim_suppliers.supplier_email, \"left\")\n",
    "      .join(dim_dates_rel, df_products_raw.release_full_date == dim_dates_rel.release_full_date, \"left\")\n",
    "      .join(dim_dates_exp, df_products_raw.expiry_full_date  == dim_dates_exp.expiry_full_date,  \"left\")\n",
    "      .select(\n",
    "          col(\"product_name\"),\n",
    "          col(\"supplier_id\"),\n",
    "          col(\"category_id\"),\n",
    "          col(\"price\"),\n",
    "          col(\"weight\"),\n",
    "          col(\"color_id\"),\n",
    "          col(\"size_id\"),\n",
    "          col(\"brand_id\"),\n",
    "          col(\"material_id\"),\n",
    "          col(\"description\"),\n",
    "          col(\"rating\"),\n",
    "          col(\"reviews\"),\n",
    "          col(\"release_date_id\"),\n",
    "          col(\"expiry_date_id\")\n",
    "      )\n",
    "      .distinct()\n",
    ")\n",
    "\n",
    "# 4) –ê–Ω—Ç–∏-–¥–∂–æ–∏–Ω, —á—Ç–æ–±—ã –Ω–µ –≤—Å—Ç–∞–≤–ª—è—Ç—å –¥—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è –∫–ª—é—á–∏ (product_name, supplier_id)\n",
    "df_exist = spark.read.jdbc(url=jdbc_url, table=\"dim_products\", properties=pg_props) \\\n",
    "                 .select(\"product_name\", \"supplier_id\")\n",
    "\n",
    "to_insert = df_products.join(df_exist, [\"product_name\", \"supplier_id\"], \"left_anti\")\n",
    "ins_cnt   = to_insert.count()\n",
    "print(f\"   ‚ûï –ë—É–¥—É—Ç –≤—Å—Ç–∞–≤–ª–µ–Ω—ã {ins_cnt} –Ω–æ–≤—ã—Ö —Å—Ç—Ä–æ–∫ –≤ dim_products\")\n",
    "\n",
    "# 5) –ó–∞–ø–∏—Å—ã–≤–∞–µ–º\n",
    "if ins_cnt > 0:\n",
    "    to_insert.write.jdbc(\n",
    "        url=jdbc_url,\n",
    "        table=\"dim_products\",\n",
    "        mode=\"append\",\n",
    "        properties=pg_props\n",
    "    )\n",
    "    print(\"   ‚úÖ –í—Å—Ç–∞–≤–∫–∞ –≤ dim_products –∑–∞–≤–µ—Ä—à–µ–Ω–∞\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è –ù–µ—á–µ–≥–æ –≤—Å—Ç–∞–≤–ª—è—Ç—å –≤ dim_products\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e79d8f38-d26b-4ac2-a99a-8907c56858d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ dim_customers...\n",
      "   üîç –ù–∞–π–¥–µ–Ω–æ –≤ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö 10000 —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤ (–ø–æ email)\n",
      "   üîç –ü–æ—Å–ª–µ –¥–∂–æ–∏–Ω–∞ —Å–æ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∞–º–∏ 16581 –∑–∞–ø–∏—Å–µ–π\n",
      "   ‚ûï –ë—É–¥—É—Ç –≤—Å—Ç–∞–≤–ª–µ–Ω—ã 10000 –Ω–æ–≤—ã—Ö —Å—Ç—Ä–æ–∫ –≤ dim_customers\n",
      "   ‚úÖ –í—Å—Ç–∞–≤–∫–∞ –≤ dim_customers –∑–∞–≤–µ—Ä—à–µ–Ω–∞\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# –§–∏–Ω–∞–ª—å–Ω—ã–π –±–ª–æ–∫: dim_customers\n",
    "# -------------------------------\n",
    "print(\"üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ dim_customers...\")\n",
    "\n",
    "# –°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ emails\n",
    "df_exist = spark.read.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"dim_customers\",\n",
    "    properties=pg_props\n",
    ").select(\"email\")\n",
    "\n",
    "# –ò—Å—Ö–æ–¥–Ω—ã–µ –∫–ª–∏–µ–Ω—Ç—ã\n",
    "df_cust_new = (\n",
    "    df_raw\n",
    "      .select(\n",
    "         col(\"customer_first_name\").alias(\"first_name\"),\n",
    "         col(\"customer_last_name\").alias(\"last_name\"),\n",
    "         col(\"customer_age\").alias(\"age\"),\n",
    "         col(\"customer_email\").alias(\"email\"),\n",
    "         col(\"customer_country\").alias(\"country_name\"),\n",
    "         col(\"customer_postal_code\").alias(\"postal_code\"),\n",
    "         col(\"customer_pet_name\").alias(\"pet_name\")\n",
    "      )\n",
    "      .where(col(\"email\").isNotNull())\n",
    "      .distinct()\n",
    ")\n",
    "\n",
    "print(f\"   üîç –ù–∞–π–¥–µ–Ω–æ –≤ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö {df_cust_new.count()} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤ (–ø–æ email)\")\n",
    "\n",
    "# –ü–æ–¥—Ç—è–≥–∏–≤–∞–µ–º FK\n",
    "dim_countries = spark.read.jdbc(url=jdbc_url, table=\"dim_countries\", properties=pg_props)\n",
    "dim_pets      = spark.read.jdbc(url=jdbc_url, table=\"dim_pets\",      properties=pg_props)\n",
    "\n",
    "df_cust = (\n",
    "    df_cust_new\n",
    "      .join(dim_countries, [\"country_name\"], \"left\")\n",
    "      .join(dim_pets,      [\"pet_name\"],      \"left\")\n",
    "      .select(\"first_name\", \"last_name\", \"age\", \"email\", \n",
    "              col(\"country_id\"), \"postal_code\", col(\"pet_id\"))\n",
    ")\n",
    "\n",
    "print(f\"   üîç –ü–æ—Å–ª–µ –¥–∂–æ–∏–Ω–∞ —Å–æ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∞–º–∏ {df_cust.count()} –∑–∞–ø–∏—Å–µ–π\")\n",
    "\n",
    "# –ê–Ω—Ç–∏-–¥–∂–æ–∏–Ω + —É–¥–∞–ª—è–µ–º –¥—É–±–ª–∏ –ø–æ email\n",
    "to_insert = (\n",
    "    df_cust.join(df_exist, [\"email\"], \"left_anti\")\n",
    "           .dropDuplicates([\"email\"])\n",
    ")\n",
    "ins_cnt = to_insert.count()\n",
    "print(f\"   ‚ûï –ë—É–¥—É—Ç –≤—Å—Ç–∞–≤–ª–µ–Ω—ã {ins_cnt} –Ω–æ–≤—ã—Ö —Å—Ç—Ä–æ–∫ –≤ dim_customers\")\n",
    "\n",
    "# –ó–∞–ø–∏—Å—å\n",
    "if ins_cnt > 0:\n",
    "    to_insert.write.jdbc(\n",
    "        url=jdbc_url,\n",
    "        table=\"dim_customers\",\n",
    "        mode=\"append\",\n",
    "        properties=pg_props\n",
    "    )\n",
    "    print(\"   ‚úÖ –í—Å—Ç–∞–≤–∫–∞ –≤ dim_customers –∑–∞–≤–µ—Ä—à–µ–Ω–∞\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è –ù–µ—á–µ–≥–æ –≤—Å—Ç–∞–≤–ª—è—Ç—å –≤ dim_customers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ddd975d4-5100-4b63-8865-07c7515b64c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ dim_sellers...\n",
      "   üîç –ù–∞–π–¥–µ–Ω–æ 10000 —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø—Ä–æ–¥–∞–≤—Ü–æ–≤ (–ø–æ email)\n",
      "   üîç –ü–æ—Å–ª–µ –¥–∂–æ–∏–Ω–∞ —Å–æ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∞–º–∏ –ø–æ–ª—É—á–µ–Ω–æ 10000 –∑–∞–ø–∏—Å–µ–π\n",
      "   ‚ûï –ë—É–¥—É—Ç –≤—Å—Ç–∞–≤–ª–µ–Ω—ã 10000 –Ω–æ–≤—ã—Ö —Å—Ç—Ä–æ–∫ –≤ dim_sellers\n",
      "   ‚úÖ –í—Å—Ç–∞–≤–∫–∞ –≤ dim_sellers –∑–∞–≤–µ—Ä—à–µ–Ω–∞\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# dim_sellers (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –±–ª–æ–∫)\n",
    "# -------------------------------\n",
    "print(\"üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ dim_sellers...\")\n",
    "\n",
    "# 1) –°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ emails –ø—Ä–æ–¥–∞–≤—Ü–æ–≤\n",
    "df_exist = spark.read.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"dim_sellers\",\n",
    "    properties=pg_props\n",
    ").select(\"email\")\n",
    "\n",
    "# 2) –ò—Å—Ö–æ–¥–Ω—ã–π DataFrame –ø—Ä–æ–¥–∞–≤—Ü–æ–≤\n",
    "df_sellers_new = (\n",
    "    df_raw\n",
    "      .select(\n",
    "         col(\"seller_first_name\").alias(\"first_name\"),\n",
    "         col(\"seller_last_name\").alias(\"last_name\"),\n",
    "         col(\"seller_email\").alias(\"email\"),\n",
    "         col(\"seller_country\").alias(\"country_name\"),\n",
    "         col(\"seller_postal_code\").alias(\"postal_code\")\n",
    "      )\n",
    "      .where(col(\"email\").isNotNull())\n",
    "      .distinct()\n",
    ")\n",
    "\n",
    "new_cnt = df_sellers_new.count()\n",
    "print(f\"   üîç –ù–∞–π–¥–µ–Ω–æ {new_cnt} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø—Ä–æ–¥–∞–≤—Ü–æ–≤ (–ø–æ email)\")\n",
    "\n",
    "# 3) –ü–æ–¥—Ç—è–≥–∏–≤–∞–µ–º –≤–Ω–µ—à–Ω–∏–π –∫–ª—é—á country_id\n",
    "dim_countries = spark.read.jdbc(url=jdbc_url, table=\"dim_countries\", properties=pg_props)\n",
    "\n",
    "df_sellers = (\n",
    "    df_sellers_new\n",
    "      .join(dim_countries, [\"country_name\"], \"left\")\n",
    "      .select(\"first_name\", \"last_name\", \"email\", col(\"country_id\"), \"postal_code\")\n",
    ")\n",
    "\n",
    "post_join_cnt = df_sellers.count()\n",
    "print(f\"   üîç –ü–æ—Å–ª–µ –¥–∂–æ–∏–Ω–∞ —Å–æ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∞–º–∏ –ø–æ–ª—É—á–µ–Ω–æ {post_join_cnt} –∑–∞–ø–∏—Å–µ–π\")\n",
    "\n",
    "# 4) –ê–Ω—Ç–∏-–¥–∂–æ–∏–Ω –∏ —É–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ email\n",
    "to_insert = (\n",
    "    df_sellers\n",
    "      .join(df_exist, [\"email\"], \"left_anti\")\n",
    "      .dropDuplicates([\"email\"])\n",
    ")\n",
    "ins_cnt = to_insert.count()\n",
    "print(f\"   ‚ûï –ë—É–¥—É—Ç –≤—Å—Ç–∞–≤–ª–µ–Ω—ã {ins_cnt} –Ω–æ–≤—ã—Ö —Å—Ç—Ä–æ–∫ –≤ dim_sellers\")\n",
    "\n",
    "# 5) –ó–∞–ø–∏—Å—å\n",
    "if ins_cnt > 0:\n",
    "    to_insert.write.jdbc(\n",
    "        url=jdbc_url,\n",
    "        table=\"dim_sellers\",\n",
    "        mode=\"append\",\n",
    "        properties=pg_props\n",
    "    )\n",
    "    print(\"   ‚úÖ –í—Å—Ç–∞–≤–∫–∞ –≤ dim_sellers –∑–∞–≤–µ—Ä—à–µ–Ω–∞\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è –ù–µ—á–µ–≥–æ –≤—Å—Ç–∞–≤–ª—è—Ç—å –≤ dim_sellers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b316f4ed-23ac-4f12-b201-af1ae1d0a11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ dim_stores...\n",
      "   üîç –í —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–∞–π–¥–µ–Ω–æ 10000 —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –º–∞–≥–∞–∑–∏–Ω–æ–≤ (–ø–æ name+location)\n",
      "   üîç –ü–æ—Å–ª–µ –¥–∂–æ–∏–Ω–∞ —Å–æ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∞–º–∏ –ø–æ–ª—É—á–µ–Ω–æ 10000 –∑–∞–ø–∏—Å–µ–π\n",
      "   ‚ûï –ë—É–¥—É—Ç –≤—Å—Ç–∞–≤–ª–µ–Ω—ã 9688 –Ω–æ–≤—ã—Ö —Å—Ç—Ä–æ–∫ –≤ dim_stores\n",
      "   ‚úÖ –í—Å—Ç–∞–≤–∫–∞ –≤ dim_stores –∑–∞–≤–µ—Ä—à–µ–Ω–∞\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# dim_stores (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –±–ª–æ–∫)\n",
    "# -------------------------------\n",
    "print(\"üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ dim_stores...\")\n",
    "\n",
    "# 1) –ß–∏—Ç–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∫–ª—é—á–∏ (store_name + location)\n",
    "df_exist = spark.read.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"dim_stores\",\n",
    "    properties=pg_props\n",
    ").select(\"store_name\", \"location\")\n",
    "\n",
    "# 2) –°–æ–±–∏—Ä–∞–µ–º –Ω–æ–≤—ã–µ –∑–∞–ø–∏—Å–∏\n",
    "df_stores_new = (\n",
    "    df_raw\n",
    "      .select(\n",
    "         col(\"store_name\"),\n",
    "         col(\"store_location\").alias(\"location\"),\n",
    "         col(\"store_city\").alias(\"city_name\"),\n",
    "         col(\"store_state\").alias(\"state\"),\n",
    "         col(\"store_country\").alias(\"country_name\"),\n",
    "         col(\"store_phone\").alias(\"phone\"),\n",
    "         col(\"store_email\").alias(\"email\")\n",
    "      )\n",
    "      .where(col(\"store_name\").isNotNull())\n",
    "      .distinct()\n",
    ")\n",
    "\n",
    "new_cnt = df_stores_new.count()\n",
    "print(f\"   üîç –í —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–∞–π–¥–µ–Ω–æ {new_cnt} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –º–∞–≥–∞–∑–∏–Ω–æ–≤ (–ø–æ name+location)\")\n",
    "\n",
    "# 3) –ü–æ–¥—Ç—è–≥–∏–≤–∞–µ–º FK city_id –∏ country_id\n",
    "dim_cities    = spark.read.jdbc(url=jdbc_url, table=\"dim_cities\",    properties=pg_props)\n",
    "dim_countries = spark.read.jdbc(url=jdbc_url, table=\"dim_countries\", properties=pg_props)\n",
    "\n",
    "df_stores = (\n",
    "    df_stores_new\n",
    "      .join(dim_cities,    [\"city_name\"],    \"left\")\n",
    "      .join(dim_countries, [\"country_name\"], \"left\")\n",
    "      .select(\n",
    "          \"store_name\",\n",
    "          \"location\",\n",
    "          col(\"city_id\"),\n",
    "          \"state\",\n",
    "          col(\"country_id\"),\n",
    "          \"phone\",\n",
    "          \"email\"\n",
    "      )\n",
    ")\n",
    "\n",
    "post_join_cnt = df_stores.count()\n",
    "print(f\"   üîç –ü–æ—Å–ª–µ –¥–∂–æ–∏–Ω–∞ —Å–æ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∞–º–∏ –ø–æ–ª—É—á–µ–Ω–æ {post_join_cnt} –∑–∞–ø–∏—Å–µ–π\")\n",
    "\n",
    "# 4) –ê–Ω—Ç–∏-–¥–∂–æ–∏–Ω –∏ —É–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ –∫–ª—é—á—É\n",
    "to_insert = (\n",
    "    df_stores\n",
    "      .join(df_exist, [\"store_name\", \"location\"], \"left_anti\")\n",
    "      .dropDuplicates([\"store_name\", \"location\"])\n",
    ")\n",
    "ins_cnt = to_insert.count()\n",
    "print(f\"   ‚ûï –ë—É–¥—É—Ç –≤—Å—Ç–∞–≤–ª–µ–Ω—ã {ins_cnt} –Ω–æ–≤—ã—Ö —Å—Ç—Ä–æ–∫ –≤ dim_stores\")\n",
    "\n",
    "# 5) –ó–∞–ø–∏—Å—å\n",
    "if ins_cnt > 0:\n",
    "    to_insert.write.jdbc(\n",
    "        url=jdbc_url,\n",
    "        table=\"dim_stores\",\n",
    "        mode=\"append\",\n",
    "        properties=pg_props\n",
    "    )\n",
    "    print(\"   ‚úÖ –í—Å—Ç–∞–≤–∫–∞ –≤ dim_stores –∑–∞–≤–µ—Ä—à–µ–Ω–∞\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è –ù–µ—á–µ–≥–æ –≤—Å—Ç–∞–≤–ª—è—Ç—å –≤ dim_stores\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b3195e65-c3f4-43e2-be47-71e9fe061a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ fact_sales...\n",
      "   üîç –ü–æ—Å–ª–µ –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –¥–∂–æ–∏–Ω—Ç–æ–≤ –Ω–∞–π–¥–µ–Ω–æ 10000 —Ñ–∞–∫—Ç–æ–≤ (–æ–∂–∏–¥–∞–µ—Ç—Å—è ‚âà10000)\n",
      "   ‚úÖ –í—Å—Ç–∞–≤–∫–∞ –≤ fact_sales –∑–∞–≤–µ—Ä—à–µ–Ω–∞\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# fact_sales\n",
    "# -------------------------------\n",
    "print(\"üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ fact_sales...\")\n",
    "\n",
    "# 1) –ß–∏—Ç–∞–µ–º –∏ ¬´—Å—É–∂–∞–µ–º¬ª –∏–∑–º–µ—Ä–µ–Ω–∏—è –¥–æ (–∫–ª—é—á, id) –∏ —É–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã\n",
    "dim_dates = (\n",
    "    spark.read.jdbc(url=jdbc_url, table=\"dim_dates\", properties=pg_props)\n",
    "         .select(\"full_date\", \"date_id\")\n",
    "         .dropDuplicates([\"full_date\"])\n",
    ")\n",
    "\n",
    "dim_customers = (\n",
    "    spark.read.jdbc(url=jdbc_url, table=\"dim_customers\", properties=pg_props)\n",
    "         .select(\"email\", \"customer_id\")\n",
    "         .dropDuplicates([\"email\"])\n",
    ")\n",
    "\n",
    "dim_sellers = (\n",
    "    spark.read.jdbc(url=jdbc_url, table=\"dim_sellers\", properties=pg_props)\n",
    "         .select(\"email\", \"seller_id\")\n",
    "         .dropDuplicates([\"email\"])\n",
    ")\n",
    "\n",
    "dim_products = (\n",
    "    spark.read.jdbc(url=jdbc_url, table=\"dim_products\", properties=pg_props)\n",
    "         .select(\"product_name\", \"product_id\")\n",
    "         .dropDuplicates([\"product_name\"])\n",
    ")\n",
    "\n",
    "dim_stores = (\n",
    "    spark.read.jdbc(url=jdbc_url, table=\"dim_stores\", properties=pg_props)\n",
    "         .select(\"store_name\", \"store_id\")\n",
    "         .dropDuplicates([\"store_name\"])\n",
    ")\n",
    "\n",
    "# 2) –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Å—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –∫–æ–ª–æ–Ω–∫–∞–º–∏ –¥–ª—è –¥–∂–æ–π–Ω–∞\n",
    "df_facts = (\n",
    "    df_raw\n",
    "      .withColumn(\"sale_dt\", to_date(\"sale_date\", \"MM/dd/yyyy\"))\n",
    "      .select(\n",
    "          \"sale_dt\",\n",
    "          \"customer_email\",\n",
    "          \"seller_email\",\n",
    "          \"product_name\",\n",
    "          \"store_name\",\n",
    "          col(\"sale_quantity\").alias(\"quantity\"),\n",
    "          col(\"sale_total_price\").alias(\"total_price\")\n",
    "      )\n",
    ")\n",
    "\n",
    "# 3) –î–∂–æ–π–Ω–∏–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ\n",
    "df_facts = (\n",
    "    df_facts\n",
    "      .join(dim_dates,     df_facts.sale_dt   == dim_dates.full_date, \"inner\")\n",
    "      .join(dim_customers, df_facts.customer_email == dim_customers.email, \"inner\")\n",
    "      .join(dim_sellers,   df_facts.seller_email   == dim_sellers.email,   \"inner\")\n",
    "      .join(dim_products,  df_facts.product_name   == dim_products.product_name, \"inner\")\n",
    "      .join(dim_stores,    df_facts.store_name     == dim_stores.store_name,     \"inner\")\n",
    "      .select(\n",
    "          \"date_id\",\n",
    "          \"customer_id\",\n",
    "          \"seller_id\",\n",
    "          \"product_id\",\n",
    "          \"store_id\",\n",
    "          \"quantity\",\n",
    "          \"total_price\"\n",
    "      )\n",
    ")\n",
    "\n",
    "count = df_facts.count()\n",
    "print(f\"   üîç –ü–æ—Å–ª–µ –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –¥–∂–æ–∏–Ω—Ç–æ–≤ –Ω–∞–π–¥–µ–Ω–æ {count} —Ñ–∞–∫—Ç–æ–≤\")\n",
    "\n",
    "# 4) –ó–∞–ø–∏—Å—ã–≤–∞–µ–º\n",
    "if count > 0:\n",
    "    df_facts.write.jdbc(\n",
    "        url=jdbc_url,\n",
    "        table=\"fact_sales\",\n",
    "        mode=\"append\",\n",
    "        properties=pg_props\n",
    "    )\n",
    "    print(\"   ‚úÖ –í—Å—Ç–∞–≤–∫–∞ –≤ fact_sales –∑–∞–≤–µ—Ä—à–µ–Ω–∞\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è –ù–µ—á–µ–≥–æ –≤—Å—Ç–∞–≤–ª—è—Ç—å –≤ fact_sales\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4282787e-2be5-4854-83f3-7a6e38214887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úîÔ∏è SparkSession –∏ JDBC –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã\n"
     ]
    }
   ],
   "source": [
    "# –Ø—á–µ–π–∫–∞ 1: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Spark –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ETL Star Schema Reports to ClickHouse\") \\\n",
    "    .config(\"spark.jars\", \"postgresql-42.6.0.jar,clickhouse-jdbc-0.4.6.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# PostgreSQL\n",
    "pg_url   = \"jdbc:postgresql://postgres:5432/spark_db\"\n",
    "pg_props = {\n",
    "    \"user\": \"spark_user\",\n",
    "    \"password\": \"spark_password\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "# ClickHouse\n",
    "ch_url   = \"jdbc:clickhouse://clickhouse:8123/default\"\n",
    "ch_props = {\n",
    "    \"driver\":   \"com.clickhouse.jdbc.ClickHouseDriver\",\n",
    "    \"user\":     \"default\",\n",
    "    \"password\": \"\",\n",
    "}\n",
    "print(\"‚úîÔ∏è SparkSession –∏ JDBC –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c8315e-c77d-4a0f-b38d-98dd9f51311e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "del spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd7c71a-39b0-41af-8f1e-e9b6e47270a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "print(SparkContext._active_spark_context)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
